{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Text Summarization Project"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n","\n","In the era of information overload, the ability to distill essential insights from large volumes of text is more crucial than ever. This is where the field of text summarization comes into play. Text summarization is a branch of Natural Language Processing (NLP) that involves condensing a larger body of text into a short, coherent summary while preserving its key informational elements and overall meaning.\n","\n","The primary goal of this project is to build a text summarization model that can take a lengthy piece of text and generate a concise summary, much like a human would. This has wide-ranging applications in numerous fields such as journalism, where it can be used to generate news digests, in academia for literature reviews, or in business for summarizing reports and meetings.\n","\n","One popular example of a text summarization tool is Quillbot, which uses advanced NLP techniques to paraphrase and summarize text. Similarly, our project aims to create a model that can understand the context and extract the salient points from a piece of text, thereby saving the reader's time and effort.\n","\n","For instance, given a long article about climate change, our model should be able to generate a summary like: \"The article discusses the increasing threat of climate change, highlighting the rise in global temperatures and sea levels. It emphasizes the need for urgent action, suggesting renewable energy and sustainable practices as potential solutions.\"\n","\n","In the following sections, we will walk through the steps of building such a model, from data collection and preprocessing to model training and evaluation. We will also discuss the challenges faced during the project and how we overcame them. Let's dive in!"]},{"cell_type":"markdown","metadata":{},"source":["### Objective\n","\n","The primary objective of this project is to design and implement an effective text summarization model that can condense lengthy textual information into a concise and coherent summary. The aim is to retain the key points and overall context of the original text in the summary, thereby providing a quick and accurate understanding of the content without the need to read through the entire text.\n","\n","In the context of this project, I am particularly interested in exploring the application of advanced Natural Language Processing (NLP) techniques and machine learning algorithms for this task. I aim to build a model that can handle a variety of text types, ranging from news articles and research papers to blog posts and book chapters.\n","\n","Another key objective is to ensure that the generated summaries are not only short and informative but also grammatically correct and readable. The model should be able to generate summaries that are smooth, coherent, and can stand on their own as a comprehensive reflection of the original text.\n","\n","Furthermore, I aim to evaluate the performance of my model rigorously and objectively, using established evaluation metrics in the field of NLP. This will allow me to understand the strengths and weaknesses of my model and guide future improvements.\n","\n","Ultimately, the goal is to contribute to the ongoing efforts in the field of NLP to make information more accessible and manageable in the face of growing data. By creating an effective text summarization model, I hope to provide a tool that can save time and effort for anyone who needs to understand large volumes of text quickly and efficiently."]},{"cell_type":"markdown","metadata":{},"source":["### Data Collection\n","\n","For this project, I used the [SAMSum Corpus](https://huggingface.co/datasets/samsum), a dataset that contains approximately 16,000 messenger-like conversations along with their summaries. These conversations were created and written down by linguists fluent in English, reflecting a variety of topics, styles, and registers. The dataset was prepared by Samsung R&D Institute Poland and is distributed for research purposes.\n","\n","### Data Preparation\n","\n","The SAMSum Corpus is already structured and annotated, which simplifies the data preparation process. However, it's still necessary to preprocess the data for the specific requirements of the model. This includes tokenizing the text and converting it into a format that can be fed into the model.\n","\n","### Model Selection\n","\n","For this project, I chose to use the pre-trained PEGASUS model provided by Google. PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive SUmmarization Sequence-to-sequence) is a state-of-the-art model for abstractive text summarization. The specific version of the model I used is '[google/pegasus-cnn_dailymail](https://huggingface.co/google/pegasus-cnn_dailymail)', which has been trained on both the C4 and HugeNews datasets.\n","\n","### Model Training\n","\n","The pre-trained PEGASUS model was then fine-tuned on the SAMSum Corpus. Fine-tuning is a process where the pre-trained model is further trained on a specific task (in this case, text summarization), allowing it to adapt to the specific characteristics of the task.\n","\n","### Model Evaluation\n","\n","After the model was fine-tuned, it was evaluated on a separate test set from the SAMSum Corpus. The performance of the model was measured using established evaluation metrics for text summarization, such as ROUGE and BLEU scores.\n","\n","### Model Deployment\n","\n","Once the model was trained and evaluated, it was deployed using AWS and GitHub Actions. AWS provides a robust platform for hosting machine learning models, while GitHub Actions allows for automated deployment processes. This ensures that any updates to the model or the code are automatically reflected in the deployed application.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":524,"status":"ok","timestamp":1690573877994,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"m5yUlhJ-8IEj","outputId":"6a4f8815-d424-47dd-bbf7-6ddccdc1d925"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Jul 28 19:51:16 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   71C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7963,"status":"ok","timestamp":1690573885951,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"B1UqBrSA8dbk"},"outputs":[],"source":["!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":51389,"status":"ok","timestamp":1690573937932,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"KNQiXOOc8mtK"},"outputs":[],"source":["\n","!pip install --upgrade accelerate -q\n","!pip uninstall -y transformers accelerate -q\n","!pip install transformers accelerate -q"]},{"cell_type":"markdown","metadata":{},"source":["### Dependencies and Setup\n","\n","Before we start with the actual model training and evaluation, we need to set up our environment and download the necessary dependencies. Here are the libraries and modules we will be using:\n","\n","- **Transformers**: This library, developed by Hugging Face, provides thousands of pre-trained models to perform tasks on texts such as classification, information extraction, summarization, etc. We will be using it to access the pre-trained PEGASUS model.\n","\n","- **Datasets**: Also developed by Hugging Face, this library provides a simple API to download and preprocess datasets. We will be using it to download the SAMSum Corpus.\n","\n","- **Matplotlib**: This is a comprehensive library for creating static, animated, and interactive visualizations in Python. We will be using it to plot the training progress.\n","\n","- **Pandas**: This library provides high-performance, easy-to-use data structures and data analysis tools for Python. We will be using it to manipulate our data.\n","\n","- **NLTK**: The Natural Language Toolkit (NLTK) is a platform used for building Python programs to work with human language data. We will be using it for tokenization.\n","\n","- **TQDM**: This library provides a fast, extensible progress bar for Python and CLI. We will be using it to visualize the progress of our loops.\n","\n","- **Torch**: PyTorch is an open-source machine learning library based on the Torch library. We will be using it as our main library to train our model.\n","\n","- **Warnings**: This is a standard Python module for warning control. We will be using it to ignore any warning messages to keep our output clean."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12588,"status":"ok","timestamp":1690573950511,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"nhpbQRit8yVJ","outputId":"0ad61295-6a84-4604-db74-a79920d7b45e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["from transformers import pipeline, set_seed\n","from datasets import load_dataset, load_from_disk\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","import pandas as pd\n","from datasets import load_dataset, load_metric\n","\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","from tqdm import tqdm\n","import torch\n","\n","nltk.download(\"punkt\")\n","\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["### Setting up the Device\n","\n","Before we start with the model training, we need to set up our device configuration. In PyTorch, we need to set up our device as either CPU or CUDA (which stands for Compute Unified Device Architecture, a parallel computing platform and application programming interface model created by NVIDIA). If a GPU is available, PyTorch will use it by default, otherwise, it will use the CPU."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1690573950513,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"pGHw-_iM9JIs","outputId":"874051ca-5cae-4151-ccf6-a8087af5f7d5"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device\n"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the Pre-trained Model and Tokenizer\n","\n","We will be using the PEGASUS model pre-trained on the CNN/DailyMail dataset. The model checkpoint is available on the Hugging Face model hub under the name 'google/pegasus-cnn_dailymail'.\n","\n","We first load the tokenizer associated with the model. The tokenizer is responsible for preprocessing the text for the model. This includes steps like tokenization, which is the process of converting the text into tokens (smaller parts like words or subwords), and encoding, which is the process of converting these tokens into numbers that the model can understand.\n","\n","Next, we load the pre-trained PEGASUS model. We specify that the model should be loaded onto our device (either the CPU or GPU, depending on availability).\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39157,"status":"ok","timestamp":1690573989661,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"f9EcFwLd9V_D","outputId":"370811e1-a994-403e-d345-fcfac32c1351"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["\n","model_ckpt = \"google/pegasus-cnn_dailymail\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","\n","model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Downloading and Unzipping the Dataset\n","\n","The SAMSum dataset is used for this project. It is available on GitHub and can be downloaded directly. After downloading, the dataset is unzipped to access the data files.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107344,"status":"ok","timestamp":1690574096988,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"SbTwf-mJ9aKB","outputId":"fa4f66d0-f97c-4891-f88d-fa1aa829952e"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-07-28 19:53:08--  https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n","Resolving github.com (github.com)... 20.205.243.166\n","Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip [following]\n","--2023-07-28 19:53:08--  https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7903594 (7.5M) [application/zip]\n","Saving to: â€˜summarizer-data.zip.1â€™\n","\n","summarizer-data.zip 100%[===================>]   7.54M  --.-KB/s    in 0.08s   \n","\n","2023-07-28 19:53:09 (92.0 MB/s) - â€˜summarizer-data.zip.1â€™ saved [7903594/7903594]\n","\n","Archive:  summarizer-data.zip\n","replace samsum-test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}],"source":["\n","#download & unzip data\n","\n","!wget https://github.com/nani2357/My_dataset_repo/raw/main/samsumdata.zip\n","!unzip summarizer-data.zip"]},{"cell_type":"markdown","metadata":{},"source":["The dataset is loaded from the disk using the `load_from_disk` function from the `datasets` library. This function reads a dataset that was previously saved using the `save_to_disk` function."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690574096989,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"VkfC9g4v-swK","outputId":"94d0e541-ef95-4243-c7bb-2e551c3fa98b"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 14732\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 819\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 818\n","    })\n","})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["\n","dataset_samsum = load_from_disk('samsum_dataset')\n","dataset_samsum\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exploring the Dataset\n","\n","To understand the dataset better, the length of each split (train, test, validation) is calculated and the features of the dataset are printed. The dataset consists of dialogues and their corresponding summaries.\n","\n","The dialogue and summary of a sample from the test set are also printed to get a sense of what the data looks like.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1690574096989,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"m3uCXN1M-0Vq","outputId":"bf5c7925-9fa4-4222-9921-bbd40225598f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Split lengths: [14732, 819, 818]\n","Features: ['id', 'dialogue', 'summary']\n","\n","Dialogue:\n","Eric: MACHINE!\r\n","Rob: That's so gr8!\r\n","Eric: I know! And shows how Americans see Russian ;)\r\n","Rob: And it's really funny!\r\n","Eric: I know! I especially like the train part!\r\n","Rob: Hahaha! No one talks to the machine like that!\r\n","Eric: Is this his only stand-up?\r\n","Rob: Idk. I'll check.\r\n","Eric: Sure.\r\n","Rob: Turns out no! There are some of his stand-ups on youtube.\r\n","Eric: Gr8! I'll watch them now!\r\n","Rob: Me too!\r\n","Eric: MACHINE!\r\n","Rob: MACHINE!\r\n","Eric: TTYL?\r\n","Rob: Sure :)\n","\n","Summary:\n","Eric and Rob are going to watch a stand-up on youtube.\n"]}],"source":["split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n","\n","print(f\"Split lengths: {split_lengths}\")\n","print(f\"Features: {dataset_samsum['train'].column_names}\")\n","print(\"\\nDialogue:\")\n","\n","print(dataset_samsum[\"test\"][1][\"dialogue\"])\n","\n","print(\"\\nSummary:\")\n","\n","print(dataset_samsum[\"test\"][1][\"summary\"])\n"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing the Data\n","\n","The data needs to be preprocessed before it can be fed into the model. This involves converting the dialogues and summaries into a format that the model can understand.\n","\n","A function `convert_examples_to_features` is defined for this purpose. This function takes a batch of examples and performs the following steps:\n","\n","1. It tokenizes the dialogues using the tokenizer's `__call__` method. The dialogues are truncated to a maximum length of 1024 tokens.\n","\n","2. It tokenizes the summaries in a similar way, but with a maximum length of 128 tokens. The tokenizer is switched to target mode using the `as_target_tokenizer` context manager. This is because the summaries are the targets that the model will be trained to predict.\n","\n","3. It returns a dictionary containing the input IDs, attention masks, and labels. The input IDs and attention masks are derived from the tokenized dialogues, and the labels are derived from the tokenized summaries.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690574096990,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"RiyMYFA9_bu5"},"outputs":[],"source":["def convert_examples_to_features(example_batch):\n","  input_encoding = tokenizer(example_batch['dialogue'],\n","                             max_length = 1024,\n","                             truncation=True\n","                             )\n","\n","  with tokenizer.as_target_tokenizer():\n","    target_encoding = tokenizer(example_batch['summary'],\n","                                max_length=128,\n","                                truncation=True\n","                                )\n","\n","  return {\n","      'input_ids':input_encoding['input_ids'],\n","      'attention_mask':input_encoding['attention_mask'],\n","      'labels' : target_encoding['input_ids']\n","  }"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["7042b6797c164d63ad163b508a0efaa8","eb75ea5c5fe24ba0a850efbebd5efc66","783c4068f06947228072e7ec333940db","0ae2816e26e34fa78f4a105642b4c935","133c978e208c437bbdaf878b3c223945","969a24f7fe9a476785e9514f62884a77","986548d65cb048d381225d407cda2995","d691ab5383f54a66a9e39fb0147b9c3a","78b24564958a41a7ac50799dd5faa94b","0c436232368241259560f17c9e2b6959","c105618717a54ce192d24713ef366d4e"]},"executionInfo":{"elapsed":657,"status":"ok","timestamp":1690574097640,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"wG9PqCe9BSfG","outputId":"6dce4441-5dfe-402e-c8db-1e0a364d968c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7042b6797c164d63ad163b508a0efaa8","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/819 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1690574097641,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"MUne640RBb5k","outputId":"e2592be7-40e7-4d95-b06f-b1b79a2bf66b"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n","    num_rows: 14732\n","})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["dataset_samsum_pt[\"train\"]"]},{"cell_type":"markdown","metadata":{},"source":["### Training the Model\n","\n","The model is trained using the Hugging Face's `Trainer` class. The `Trainer` requires a number of arguments:\n","\n","1. `model`: The model to be trained, which in this case is the Pegasus model.\n","\n","2. `args`: Training arguments that specify the training parameters. The `TrainingArguments` class is used to create the training arguments.\n","\n","3. `data_collator`: The data collator is responsible for batching the data. The `DataCollatorForSeq2Seq` class is used to create the data collator.\n","\n","4. `tokenizer`: The tokenizer used for preprocessing the data.\n","\n","5. `train_dataset` and `eval_dataset`: The training and validation datasets.\n","\n","The `Trainer` is then used to train the model with the `train` method.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1690574097642,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"P0G3Esm5CCS-"},"outputs":[],"source":["#training\n","\n","from transformers import DataCollatorForSeq2Seq\n","\n","seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1690574097642,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"8DIbkW69CayT"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","trainer_args = TrainingArguments(\n","    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n","    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n","    weight_decay=0.01, logging_steps=10,\n","    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n","    gradient_accumulation_steps=16\n",")\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1690574097643,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"n34pW9UYCm-4"},"outputs":[],"source":["trainer = Trainer(model=model_pegasus, args=trainer_args,\n","                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n","                  train_dataset=dataset_samsum_pt[\"test\"],\n","                  eval_dataset=dataset_samsum_pt[\"validation\"])\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"executionInfo":{"elapsed":159724,"status":"ok","timestamp":1690574257356,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"oIkcT6HcCv7b","outputId":"fe2e03b8-c50b-4032-e04e-476b377919f1"},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [51/51 02:35, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=51, training_loss=3.0761698928533816, metrics={'train_runtime': 159.3528, 'train_samples_per_second': 5.14, 'train_steps_per_second': 0.32, 'total_flos': 313317832187904.0, 'train_loss': 3.0761698928533816, 'epoch': 1.0})"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluating the Model\n","\n","The model is evaluated using the ROUGE metric, which is a set of metrics used to evaluate automatic summarization and machine translation. The `load_metric` function from the `datasets` library is used to load the ROUGE metric.\n","\n","The evaluation process involves the following steps:\n","\n","1. **Batching the Data**: The test data is divided into smaller batches using the `generate_batch_sized_chunks` function. This function takes a list of elements and a batch size as input and yields successive batch-sized chunks from the list of elements.\n","\n","2. **Generating Summaries**: For each batch of articles, the model generates a batch of summaries. The `generate` method of the model is used for this purpose. The inputs to the model are tokenized versions of the articles. The `generate` method also takes a few additional parameters such as `length_penalty`, `num_beams`, and `max_length` to control the generation process.\n","\n","3. **Decoding the Summaries**: The generated summaries are then decoded using the tokenizer. The `decode` method of the tokenizer is used for this purpose. The `skip_special_tokens` and `clean_up_tokenization_spaces` parameters are set to `True` to remove any special tokens and clean up the tokenization spaces.\n","\n","4. **Calculating the Metric**: The decoded summaries and the target summaries are then added to the metric using the `add_batch` method. Finally, the metric is computed using the `compute` method.\n","\n","Here is the code for evaluating the model:\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":408,"status":"ok","timestamp":1690574294338,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"M-hO8Fm6C3wL"},"outputs":[],"source":["# Evaluation\n","\n","def generate_batch_sized_chunks(list_of_elements, batch_size):\n","    \"\"\"split the dataset into smaller batches that we can process simultaneously\n","    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n","    for i in range(0, len(list_of_elements), batch_size):\n","        yield list_of_elements[i : i + batch_size]\n","\n","\n","\n","def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n","                               batch_size=16, device=device,\n","                               column_text=\"article\",\n","                               column_summary=\"highlights\"):\n","    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n","    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n","\n","    for article_batch, target_batch in tqdm(\n","        zip(article_batches, target_batches), total=len(article_batches)):\n","\n","        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n","                        padding=\"max_length\", return_tensors=\"pt\")\n","\n","        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n","                         attention_mask=inputs[\"attention_mask\"].to(device),\n","                         length_penalty=0.8, num_beams=8, max_length=128)\n","        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n","\n","        # Finally, we decode the generated texts,\n","        # replace the  token, and add the decoded texts with the references to the metric.\n","        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n","                                clean_up_tokenization_spaces=True)\n","               for s in summaries]\n","\n","        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n","\n","\n","        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n","\n","    #  Finally compute and return the ROUGE scores.\n","    score = metric.compute()\n","    return score"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["5d47e965dc624a749ac1768a13086069","e67f9295e8f147bfa56b6e13b529fe05","dc89c9c5e961410589d06400a512c069","e309286897ef4963a5e17d14dc0c703d","0692cd6547f84416aa7ab20ec3eef921","8b598db6d56c41e8891694f9ccf5c913","192998f54cbf49d3b9894bdab1128f02","c6cefb5a8ed948719117d9d825eff128","6680b692549d4bf682825c5a81a70e61","5b91725fedce46808dd37fa6164e98ec","f0d5e6c0cbb64554b1b5d3c9b8f98099"]},"executionInfo":{"elapsed":2731,"status":"ok","timestamp":1690574308501,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"Bj_15ZDLFIgH","outputId":"26136781-062e-407d-e06f-51ebf09fb6ca"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d47e965dc624a749ac1768a13086069","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n","rouge_metric = load_metric('rouge')"]},{"cell_type":"markdown","metadata":{},"source":["### Calculating the Score and Saving the Model\n","\n","The `calculate_metric_on_test_ds` function is used to calculate the ROUGE score on the test dataset. The function takes the test dataset, the ROUGE metric, the model, the tokenizer, the batch size, and the column names for the dialogue and summary as input. The function returns a dictionary with the ROUGE scores.\n","\n","The ROUGE scores are then converted into a dictionary and displayed in a pandas DataFrame for easy visualization. The keys of the dictionary are the names of the ROUGE metrics, and the values are the corresponding scores.\n","\n","Finally, the trained model is saved using the `save_pretrained` method of the model. The model is saved in a directory named \"pegasus-samsum-model\".\n","\n","Here is the code for calculating the score and saving the model:"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98},"executionInfo":{"elapsed":36079,"status":"ok","timestamp":1690574356325,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"9iQV9a3vFLZ1","outputId":"a90c2806-b1ae-48a5-9a95-9c02be324554"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:35<00:00,  7.05s/it]\n"]},{"data":{"text/html":["\n","\n","  <div id=\"df-73993ec5-ed6d-4c83-9271-4cd8f4f4b825\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rouge1</th>\n","      <th>rouge2</th>\n","      <th>rougeL</th>\n","      <th>rougeLsum</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>pegasus</th>\n","      <td>0.017471</td>\n","      <td>0.0</td>\n","      <td>0.01512</td>\n","      <td>0.014961</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73993ec5-ed6d-4c83-9271-4cd8f4f4b825')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-16451b2e-d0f7-43b6-82ea-1b9e6f52dd15\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-16451b2e-d0f7-43b6-82ea-1b9e6f52dd15')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-16451b2e-d0f7-43b6-82ea-1b9e6f52dd15 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-73993ec5-ed6d-4c83-9271-4cd8f4f4b825 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-73993ec5-ed6d-4c83-9271-4cd8f4f4b825');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"],"text/plain":["           rouge1  rouge2   rougeL  rougeLsum\n","pegasus  0.017471     0.0  0.01512   0.014961"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["\n","score = calculate_metric_on_test_ds(\n","    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",")\n","\n","rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n","\n","pd.DataFrame(rouge_dict, index = [f'pegasus'] )"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":25584,"status":"ok","timestamp":1690574407882,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"wd1irfj7FO7x"},"outputs":[],"source":["## Save model\n","model_pegasus.save_pretrained(\"pegasus-samsum-model\")"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":692,"status":"ok","timestamp":1690574408571,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"whRXqWypFSaj","outputId":"e86bff23-f714-4d44-fe23-e939bc4a081f"},"outputs":[{"data":{"text/plain":["('tokenizer/tokenizer_config.json',\n"," 'tokenizer/special_tokens_map.json',\n"," 'tokenizer/spiece.model',\n"," 'tokenizer/added_tokens.json',\n"," 'tokenizer/tokenizer.json')"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["\n","## Save tokenizer\n","tokenizer.save_pretrained(\"tokenizer\")"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690574408571,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"EB_4Do3MFVLb"},"outputs":[],"source":["\n","#Load\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction\n","\n","The model's performance is evaluated by generating a summary for a sample dialogue from the test dataset. The `pipeline` function from the transformers library is used to create a summarization pipeline with the trained model and tokenizer.\n","\n","The `length_penalty` parameter in the `gen_kwargs` dictionary is used to control the length of the generated summary. A lower value results in shorter summaries, while a higher value results in longer summaries. The `num_beams` parameter is used for beam search, which is a search algorithm that considers multiple hypotheses at each step. The `max_length` parameter is used to limit the maximum length of the generated summary.\n","\n","The dialogue, the reference summary, and the model's generated summary are then printed for comparison.\n","\n","Here is the code for generating a summary with the model:"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48982,"status":"ok","timestamp":1690574457548,"user":{"displayName":"nani Naveen","userId":"05690016534242977346"},"user_tz":-60},"id":"wouunQONFW6q","outputId":"95449125-1619-478c-fdc0-d32017366bbb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"]},{"name":"stdout","output_type":"stream","text":["Dialogue:\n","Hannah: Hey, do you have Betty's number?\n","Amanda: Lemme check\n","Hannah: <file_gif>\n","Amanda: Sorry, can't find it.\n","Amanda: Ask Larry\n","Amanda: He called her last time we were at the park together\n","Hannah: I don't know him well\n","Hannah: <file_gif>\n","Amanda: Don't be shy, he's very nice\n","Hannah: If you say so..\n","Hannah: I'd rather you texted him\n","Amanda: Just text him ðŸ™‚\n","Hannah: Urgh.. Alright\n","Hannah: Bye\n","Amanda: Bye bye\n","\n","Reference Summary:\n","Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n","\n","Model Summary:\n","Amanda: Ask Larry Amanda: He called her last time we were at the park together .<n>Hannah: I'd rather you texted him .<n>Amanda: Just text him .\n"]}],"source":["\n","#Prediction\n","\n","gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n","\n","\n","\n","sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n","\n","reference = dataset_samsum[\"test\"][0][\"summary\"]\n","\n","pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n","\n","##\n","print(\"Dialogue:\")\n","print(sample_text)\n","\n","\n","print(\"\\nReference Summary:\")\n","print(reference)\n","\n","\n","print(\"\\nModel Summary:\")\n","print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"]},{"cell_type":"markdown","metadata":{},"source":["## FastAPI\n","\n","This script is used to create a FastAPI application that serves as a web server for the text summarization model. The application provides two endpoints: one for training the model and another for generating summaries.\n","\n","Here is a brief explanation of the code:\n","\n","1. The script begins by importing the necessary modules and initializing a FastAPI application.\n","\n","2. The `@app.get(\"/\")` decorator creates a root endpoint that redirects users to the API documentation.\n","\n","3. The `@app.get(\"/train\")` decorator creates an endpoint that trains the model when accessed. The training process is initiated by running the `main.py` script using the `os.system` function. If the training is successful, the endpoint returns a success message. If an error occurs during training, the endpoint returns an error message.\n","\n","4. The `@app.post(\"/predict\")` decorator creates an endpoint that generates a summary for a given text. The endpoint receives the text as a POST request, creates an instance of the `PredictionPipeline` class, and calls its `predict` method to generate the summary. If an error occurs during prediction, the endpoint raises an exception.\n","\n","5. The `if __name__==\"__main__\":` block runs the application on a local server at port 8080 when the script is run directly.\n","\n","This FastAPI application provides a simple and efficient way to train the model and generate summaries using HTTP requests. It can be easily integrated into other applications or services.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pjto23aCFfnb"},"outputs":[],"source":["from fastapi import FastAPI\n","import uvicorn\n","import sys\n","import os\n","from fastapi.templating import Jinja2Templates\n","from starlette.responses import RedirectResponse\n","from fastapi.responses import Response\n","from textSummarization.pipeline.prediction import PredictionPipeline\n","\n","\n","text:str = \"What is Text Summarization?\"\n","\n","app = FastAPI()\n","\n","@app.get(\"/\", tags=[\"authentication\"])\n","async def index():\n","    return RedirectResponse(url=\"/docs\")\n","\n","\n","@app.get(\"/train\")\n","async def training():\n","    try:\n","        os.system(\"python main.py\")\n","        return Response(\"Training successful !!\")\n","\n","    except Exception as e:\n","        return Response(f\"Error Occurred! {e}\")\n","    \n","    \n","@app.post(\"/predict\")\n","async def predict_route(text):\n","    try:\n","\n","        obj = PredictionPipeline()\n","        text = obj.predict(text)\n","        return text\n","    except Exception as e:\n","        raise e\n","    \n","\n","if __name__==\"__main__\":\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMP3KtfqtGuyjAyRB6g+9WV","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0692cd6547f84416aa7ab20ec3eef921":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ae2816e26e34fa78f4a105642b4c935":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c436232368241259560f17c9e2b6959","placeholder":"â€‹","style":"IPY_MODEL_c105618717a54ce192d24713ef366d4e","value":" 819/819 [00:00&lt;00:00, 1820.76 examples/s]"}},"0c436232368241259560f17c9e2b6959":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"133c978e208c437bbdaf878b3c223945":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"192998f54cbf49d3b9894bdab1128f02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b91725fedce46808dd37fa6164e98ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d47e965dc624a749ac1768a13086069":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e67f9295e8f147bfa56b6e13b529fe05","IPY_MODEL_dc89c9c5e961410589d06400a512c069","IPY_MODEL_e309286897ef4963a5e17d14dc0c703d"],"layout":"IPY_MODEL_0692cd6547f84416aa7ab20ec3eef921"}},"6680b692549d4bf682825c5a81a70e61":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7042b6797c164d63ad163b508a0efaa8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb75ea5c5fe24ba0a850efbebd5efc66","IPY_MODEL_783c4068f06947228072e7ec333940db","IPY_MODEL_0ae2816e26e34fa78f4a105642b4c935"],"layout":"IPY_MODEL_133c978e208c437bbdaf878b3c223945"}},"783c4068f06947228072e7ec333940db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d691ab5383f54a66a9e39fb0147b9c3a","max":819,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78b24564958a41a7ac50799dd5faa94b","value":819}},"78b24564958a41a7ac50799dd5faa94b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b598db6d56c41e8891694f9ccf5c913":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"969a24f7fe9a476785e9514f62884a77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"986548d65cb048d381225d407cda2995":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c105618717a54ce192d24713ef366d4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6cefb5a8ed948719117d9d825eff128":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d691ab5383f54a66a9e39fb0147b9c3a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc89c9c5e961410589d06400a512c069":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6cefb5a8ed948719117d9d825eff128","max":2169,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6680b692549d4bf682825c5a81a70e61","value":2169}},"e309286897ef4963a5e17d14dc0c703d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b91725fedce46808dd37fa6164e98ec","placeholder":"â€‹","style":"IPY_MODEL_f0d5e6c0cbb64554b1b5d3c9b8f98099","value":" 5.65k/? [00:00&lt;00:00, 137kB/s]"}},"e67f9295e8f147bfa56b6e13b529fe05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b598db6d56c41e8891694f9ccf5c913","placeholder":"â€‹","style":"IPY_MODEL_192998f54cbf49d3b9894bdab1128f02","value":"Downloading builder script: "}},"eb75ea5c5fe24ba0a850efbebd5efc66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_969a24f7fe9a476785e9514f62884a77","placeholder":"â€‹","style":"IPY_MODEL_986548d65cb048d381225d407cda2995","value":"Map: 100%"}},"f0d5e6c0cbb64554b1b5d3c9b8f98099":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
